<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-use_cases/question_answering/how_to/local_retrieval_qa">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Use local LLMs | ü¶úÔ∏èüîó Langchain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://python.langchain.com/img/parrot-chainlink-icon.png"><meta data-rh="true" name="twitter:image" content="https://python.langchain.com/img/parrot-chainlink-icon.png"><meta data-rh="true" property="og:url" content="https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Use local LLMs | ü¶úÔ∏èüîó Langchain"><meta data-rh="true" name="description" content="The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally."><meta data-rh="true" property="og:description" content="The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa"><link data-rh="true" rel="alternate" href="https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa" hreflang="en"><link data-rh="true" rel="alternate" href="https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.129f27fa.css">
<link rel="preload" href="/assets/js/runtime~main.ba678a8c.js" as="script">
<link rel="preload" href="/assets/js/main.ac0c2c80.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">ü¶úÔ∏èüîó LangChain</b></a><a class="navbar__item navbar__link" href="/docs/get_started/introduction">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/use_cases">Use cases</a><a class="navbar__item navbar__link" href="/docs/integrations">Integrations</a><a href="https://api.python.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://smith.langchain.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LangSmith</a><a href="https://js.langchain.com/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">JS/TS Docs</a><a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="mendable-search"><button class="ms-global search-btn__input ms-m-0 ms-flex ms-h-10 ms-w-full ms-flex-row ms-items-center ms-justify-between ms-gap-2 ms-rounded-xl ms-bg-gray-50 ms-p-1 ms-pl-2 ms-pr-1 ms-shadow ms-outline-none ms-ring-0 ms-transition-all hover:ms-cursor-pointer hover:ms-shadow-md sm:ms-p-2 sm:ms-pl-4 sm:ms-pr-2"><div class="ms-global search-btn__input-container ms-flex ms-w-full ms-min-w-0 ms-items-center ms-gap-1"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="search-btn__icon hover:fill-white ms-h-5 ms-w-5 ms-fill-gray-400 ms-text-gray-400 hover:ms-text-white focus:ms-fill-white focus:ms-text-white sm:ms-h-6 sm:ms-w-6" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path fill="none" d="M0 0h24v24H0z"></path><path d="M15.5 14h-.79l-.28-.27A6.471 6.471 0 0016 9.5 6.5 6.5 0 109.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path></svg><input readonly="" id="userInput" name="userInput" maxlength="512" placeholder="Search..." class="ms-global search-btn__input ms-w-full ms-flex-grow ms-truncate ms-bg-transparent ms-text-sm ms-outline-none ms-ring-0 ms-ring-transparent hover:ms-cursor-text focus:ms-outline-none focus:ms-ring-0 focus:ms-ring-transparent sm:ms-text-base"></div><div class="search-btn__shortcut ms-z-10 ms-flex ms-flex-row ms-items-center ms-gap-1 ms-rounded-lg ms-border ms-py-[2px] ms-px-[8px] ms-text-xs ms-text-gray-400 ms-transition-all disabled:ms-bg-opacity-10"><span>CTRL</span><span>K</span></div></button><div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active" href="/docs/use_cases">Use cases</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/use_cases/question_answering/">QA over Documents</a><button aria-label="Toggle the collapsible sidebar category &#x27;QA over Documents&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/use_cases/question_answering/how_to/vector_db_qa">How to</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/vector_db_qa">QA using a Retriever</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/chat_vector_db">Store and reference chat history</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/use_cases/question_answering/how_to/code/">Code understanding</a><button aria-label="Toggle the collapsible sidebar category &#x27;Code understanding&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/analyze_document">Analyze Document</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/conversational_retrieval_agents">Conversational Retrieval Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/document-context-aware-QA">Perform context-aware text splitting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/flare">Retrieve as you generate with FLARE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/hyde">Improve document indexing with HyDE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/use_cases/question_answering/how_to/local_retrieval_qa">Use local LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/multi_retrieval_qa_router">Dynamically select from multiple retrievers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/multiple_retrieval">Multiple Retrieval Sources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/qa_citations">Cite sources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/question_answering">QA over in-memory documents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/question_answering/how_to/vector_db_text_generation">Retrieve from vector stores directly</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/use_cases/question_answering/integrations/openai_functions_retrieval_qa">Integration-specific</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/use_cases/agent_simulations/">Agent simulations</a><button aria-label="Toggle the collapsible sidebar category &#x27;Agent simulations&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/use_cases/agents/">Agents</a><button aria-label="Toggle the collapsible sidebar category &#x27;Agents&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/use_cases/autonomous_agents/">Autonomous (long-running) agents</a><button aria-label="Toggle the collapsible sidebar category &#x27;Autonomous (long-running) agents&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/use_cases/multi_modal/image_agent">Multi-modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/use_cases/more/code_writing/">More</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/web_scraping/">Web Scraping</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/apis">Interacting with APIs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/chatbots">Chatbots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/code_understanding">Code Understanding</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/extraction">Extraction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/sql">SQL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/summarization">Summarization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/tagging">Tagging</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/use_cases/web_scraping">Web Scraping</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/use_cases"><span itemprop="name">Use cases</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/use_cases/question_answering/"><span itemprop="name">QA over Documents</span></a><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">How to</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Use local LLMs</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Use local LLMs</h1><p>The popularity of projects like <a href="https://github.com/imartinez/privateGPT" target="_blank" rel="noopener noreferrer">PrivateGPT</a>, <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>, and <a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener noreferrer">GPT4All</a> underscore the importance of running LLMs locally.</p><p>LangChain has <a href="https://integrations.langchain.com/" target="_blank" rel="noopener noreferrer">integrations</a> with many open source LLMs that can be run locally.</p><p>For example, here we show how to run <code>GPT4All</code> or <code>Llama-v2</code> locally (e.g., on your laptop) using local embeddings and a local LLM.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="document-loading">Document Loading<a href="#document-loading" class="hash-link" aria-label="Direct link to Document Loading" title="Direct link to Document Loading">‚Äã</a></h2><p>First, install packages needed for local embeddings and vector storage.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">pip </span><span class="token function" style="color:rgb(0, 0, 255)">install</span><span class="token plain"> gpt4all</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"> pip </span><span class="token function" style="color:rgb(0, 0, 255)">install</span><span class="token plain"> chromadb</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Load and split an example docucment.</p><p>We&#x27;ll use a blog post on agents as an example.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">document_loaders </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> WebBaseLoader</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">loader </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> WebBaseLoader</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">data </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> loader</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">text_splitter </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> RecursiveCharacterTextSplitter</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">text_splitter </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> RecursiveCharacterTextSplitter</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">chunk_size</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">500</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> chunk_overlap</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">all_splits </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> text_splitter</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">split_documents</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">data</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html"><span>WebBaseLoader</span></a> <!-- -->from <code>langchain.document_loaders</code></li><li><a href="https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html"><span>RecursiveCharacterTextSplitter</span></a> <!-- -->from <code>langchain.text_splitter</code></li></ul></div><p>Next, the below steps will download the <code>GPT4All</code> embeddings locally (if you don&#x27;t already have them).</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">vectorstores </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> Chroma</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">embeddings </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> GPT4AllEmbeddings</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">vectorstore </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> Chroma</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_documents</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">documents</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">all_splits</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> embedding</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">GPT4AllEmbeddings</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html"><span>Chroma</span></a> <!-- -->from <code>langchain.vectorstores</code></li><li><a href="https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.gpt4all.GPT4AllEmbeddings.html"><span>GPT4AllEmbeddings</span></a> <!-- -->from <code>langchain.embeddings</code></li></ul></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><p>Test similarity search is working with our local embeddings.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">question </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;What are the approaches to Task Decomposition?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">docs </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> vectorstore</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">similarity_search</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">question</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token builtin" style="color:rgb(0, 112, 193)">len</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">docs</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    4</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">docs</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Document(page_content=&#x27;Task decomposition can be done (1) by LLM with simple prompting like &quot;Steps for XYZ.\\n1.&quot;, &quot;What are the subgoals for achieving XYZ?&quot;, (2) by using task-specific instructions; e.g. &quot;Write a story outline.&quot; for writing a novel, or (3) with human inputs.&#x27;, metadata={&#x27;description&#x27;: &#x27;Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:&#x27;, &#x27;language&#x27;: &#x27;en&#x27;, &#x27;source&#x27;: &#x27;https://lilianweng.github.io/posts/2023-06-23-agent/&#x27;, &#x27;title&#x27;: &quot;LLM Powered Autonomous Agents | Lil&#x27;Log&quot;})</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="model">Model<a href="#model" class="hash-link" aria-label="Direct link to Model" title="Direct link to Model">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-v2">Llama-v2<a href="#llama-v2" class="hash-link" aria-label="Direct link to Llama-v2" title="Direct link to Llama-v2">‚Äã</a></h3><p>Download a GGML converted model (e.g., <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main" target="_blank" rel="noopener noreferrer">here</a>).</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">pip </span><span class="token function" style="color:rgb(0, 0, 255)">install</span><span class="token plain"> llama-cpp-python</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>To enable use of GPU on Apple Silicon, follow the steps <a href="https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md" target="_blank" rel="noopener noreferrer">here</a> to use the Python binding <code>with Metal support</code>.</p><p>In particular, ensure that <code>conda</code> is using the correct virtual enviorment that you created (<code>miniforge3</code>).</p><p>E.g., for me:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">conda activate /Users/rlm/miniforge3/envs/llama</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>With this confirmed:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token assign-left variable" style="color:rgb(9, 134, 88)">CMAKE_ARGS</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;-DLLAMA_METAL=on&quot;</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(9, 134, 88)">FORCE_CMAKE</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">1</span><span class="token plain"> pip </span><span class="token function" style="color:rgb(0, 0, 255)">install</span><span class="token plain"> -U llama-cpp-python --no-cache-dir</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">llms </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> LlamaCpp</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">callbacks</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">manager </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> CallbackManager</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">callbacks</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">streaming_stdout </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> StreamingStdOutCallbackHandler</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html"><span>LlamaCpp</span></a> <!-- -->from <code>langchain.llms</code></li><li><a href="https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html"><span>CallbackManager</span></a> <!-- -->from <code>langchain.callbacks.manager</code></li><li><a href="https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html"><span>StreamingStdOutCallbackHandler</span></a> <!-- -->from <code>langchain.callbacks.streaming_stdout</code></li></ul></div><p>Setting model parameters as noted in the <a href="https://python.langchain.com/docs/integrations/llms/llamacpp" target="_blank" rel="noopener noreferrer">llama.cpp docs</a>.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">n_gpu_layers </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(9, 134, 88)">1</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># Metal set to 1 is enough.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">n_batch </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(9, 134, 88)">512</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">callback_manager </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> CallbackManager</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">StreamingStdOutCallbackHandler</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Make sure the model path is correct for your system!</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">llm </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> LlamaCpp</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    model_path</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    n_gpu_layers</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">n_gpu_layers</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    n_batch</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">n_batch</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    n_ctx</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">2048</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    f16_kv</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># MUST set to True, otherwise you will run into problem after a couple of calls</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    callback_manager</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">callback_manager</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    verbose</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: format     = ggjt v3 (latest)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_vocab    = 32000</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_ctx      = 2048</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_embd     = 5120</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_mult     = 256</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_head     = 40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_layer    = 40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_rot      = 128</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: freq_base  = 10000.0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: freq_scale = 1</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: ftype      = 2 (mostly Q4_0)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_ff       = 13824</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: model size = 13B</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: ggml ctx size =    0.09 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: mem required  = 8819.71 MB (+ 1608.00 MB per state)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_new_context_with_model: kv self size  = 1600.00 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: allocating</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: using MPS</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loading &#x27;/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal&#x27;</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_add                            0x76add7460</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul                            0x76add5090</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_row                        0x76addae00</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_scale                          0x76adb2940</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_silu                           0x76adb8610</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_relu                           0x76addb700</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_gelu                           0x76addc100</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_soft_max                       0x76addcb80</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_diag_mask_inf                  0x76addd600</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_f16                   0x295f16380</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_0                  0x295f165e0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_1                  0x295f16840</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q2_K                  0x295f16aa0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q3_K                  0x295f16d00</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_K                  0x295f16f60</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q5_K                  0x295f171c0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q6_K                  0x295f17420</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_rms_norm                       0x295f17680</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_norm                           0x295f178e0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x295f17b40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x295f17da0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x295f18000</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x7962b9900</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x7962bf5f0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x7962bc630</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x142045960</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x7962ba2b0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_rope                           0x7962c35f0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_alibi_f32                      0x7962c30b0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f32_f16                    0x7962c15b0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f32_f32                    0x7962beb10</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f16_f16                    0x7962bf060</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: hasUnifiedMemory             = true</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: maxTransferRate              = built-in GPU</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;data            &#x27; buffer, size =  6984.06 MB, (35852.94 / 21845.34), warning: current allocated size is greater than the recommended max working set size</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;eval            &#x27; buffer, size =  1026.00 MB, (36878.94 / 21845.34), warning: current allocated size is greater than the recommended max working set size</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;kv              &#x27; buffer, size =  1602.00 MB, (38480.94 / 21845.34), warning: current allocated size is greater than the recommended max working set size</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;scr0            &#x27; buffer, size =   298.00 MB, (38778.94 / 21845.34), warning: current allocated size is greater than the recommended max working set size</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;scr1            &#x27; buffer, size =   512.00 MB, (39290.94 / 21845.34), warning: current allocated size is greater than the recommended max working set size</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><p>Note that these indicate that <a href="https://python.langchain.com/docs/integrations/llms/llamacpp" target="_blank" rel="noopener noreferrer">Metal was enabled properly</a>:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">ggml_metal_init: allocating</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">ggml_metal_init: using MPS</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">Question: A rap battle between Stephen Colbert and John Oliver</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Llama.generate: prefix-match hit</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    Setting: The Late Show with Stephen Colbert. The studio audience is filled with fans of both comedians, and the energy is electric. The two comedians are seated at a table, ready to begin their epic rap battle.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    Stephen Colbert: (smirking) Oh, you think you can take me down, John? You&#x27;re just a Brit with a funny accent, and I&#x27;m the king of comedy!</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    John Oliver: (grinning) Oh, you think you&#x27;re tough, Stephen? You&#x27;re just a has-been from South Carolina, and I&#x27;m the future of comedy!</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    The battle begins, with each comedian delivering clever rhymes and witty insults. Here are a few lines that might be included:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    Stephen Colbert: (rapping) You may have a big brain, John, but you can&#x27;t touch my charm / I&#x27;ve got the audience in stitches, while you&#x27;re just a blemish on the screen / Your accent is so thick, it&#x27;s like trying to hear a speech through a mouthful of marshmallows / You may have</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        load time =  2201.54 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:      sample time =   182.54 ms /   256 runs   (    0.71 ms per token,  1402.41 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        eval time =  8484.62 ms /   256 runs   (   33.14 ms per token,    30.17 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:       total time =  9000.62 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    &quot;\nSetting: The Late Show with Stephen Colbert. The studio audience is filled with fans of both comedians, and the energy is electric. The two comedians are seated at a table, ready to begin their epic rap battle.\n\nStephen Colbert: (smirking) Oh, you think you can take me down, John? You&#x27;re just a Brit with a funny accent, and I&#x27;m the king of comedy!\nJohn Oliver: (grinning) Oh, you think you&#x27;re tough, Stephen? You&#x27;re just a has-been from South Carolina, and I&#x27;m the future of comedy!\nThe battle begins, with each comedian delivering clever rhymes and witty insults. Here are a few lines that might be included:\nStephen Colbert: (rapping) You may have a big brain, John, but you can&#x27;t touch my charm / I&#x27;ve got the audience in stitches, while you&#x27;re just a blemish on the screen / Your accent is so thick, it&#x27;s like trying to hear a speech through a mouthful of marshmallows / You may have&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpt4all">GPT4All<a href="#gpt4all" class="hash-link" aria-label="Direct link to GPT4All" title="Direct link to GPT4All">‚Äã</a></h3><p>Similarly, we can use <code>GPT4All</code>.</p><p><a href="https://python.langchain.com/docs/integrations/llms/gpt4all" target="_blank" rel="noopener noreferrer">Download the GPT4All model binary</a>.</p><p>The Model Explorer on the <a href="https://gpt4all.io/index.html" target="_blank" rel="noopener noreferrer">GPT4All</a> is a great way to choose and download a model.</p><p>Then, specify the path that you downloaded to to.</p><p>E.g., for me, the model lives here:</p><p><code>/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin</code></p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">llms </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> GPT4All</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">llm </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> GPT4All</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    model</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    max_tokens</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">2048</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/llms/langchain.llms.gpt4all.GPT4All.html"><span>GPT4All</span></a> <!-- -->from <code>langchain.llms</code></li></ul></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Found model file at  /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    objc[47842]: Class GGMLMetalClass is implemented in both /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x29f48c208) and /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x29f970208). One of the two will be used. Which one is undefined.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama.cpp: using Metal</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama.cpp: loading model from /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: format     = ggjt v3 (latest)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_vocab    = 32001</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_ctx      = 2048</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_embd     = 5120</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_mult     = 256</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_head     = 40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_layer    = 40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_rot      = 128</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: ftype      = 2 (mostly Q4_0)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_ff       = 13824</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: n_parts    = 1</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: model size = 13B</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: ggml ctx size =    0.09 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_model_load_internal: mem required  = 9031.71 MB (+ 1608.00 MB per state)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_new_context_with_model: kv self size  = 1600.00 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: allocating</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: using MPS</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loading &#x27;/Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal&#x27;</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_add                            0x115fcbfb0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul                            0x115fcd4a0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_row                        0x115fce850</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_scale                          0x115fcd700</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_silu                           0x115fcd960</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_relu                           0x115fcfd50</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_gelu                           0x115fd03c0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_soft_max                       0x115fcf640</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_diag_mask_inf                  0x115fd07f0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_f16                   0x1147b2450</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11479d1d0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1147ad1f0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q2_k                  0x1147aef50</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q3_k                  0x1147af1b0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q4_k                  0x1147af410</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q5_k                  0x1147affa0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1147b0200</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_rms_norm                       0x1147b0460</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_norm                           0x1147bfc90</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x1147c0230</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1147c0490</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x1147c06f0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x1147c0950</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x1147c0bb0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x1147c0e10</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x1147c1070</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x1147c13d0</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_rope                           0x1147c1a00</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_alibi_f32                      0x1147c2120</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f32_f16                    0x115fd1690</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f32_f32                    0x115fd1c60</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: loaded kernel_cpy_f16_f16                    0x115fd2d40</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: hasUnifiedMemory             = true</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_init: maxTransferRate              = built-in GPU</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;data            &#x27; buffer, size =  6984.06 MB, ( 6984.45 / 21845.34)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;eval            &#x27; buffer, size =  1024.00 MB, ( 8008.45 / 21845.34)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;kv              &#x27; buffer, size =  1602.00 MB, ( 9610.45 / 21845.34)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;scr0            &#x27; buffer, size =   512.00 MB, (10122.45 / 21845.34)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    ggml_metal_add_buffer: allocated &#x27;scr1            &#x27; buffer, size =   512.00 MB, (10634.45 / 21845.34)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llmchain">LLMChain<a href="#llmchain" class="hash-link" aria-label="Direct link to LLMChain" title="Direct link to LLMChain">‚Äã</a></h2><p>Run an <code>LLMChain</code> (see <a href="https://python.langchain.com/docs/modules/chains/foundational/llm_chain" target="_blank" rel="noopener noreferrer">here</a>) with either model by passing in the retrieved docs and a simple prompt.</p><p>It formats the prompt template using the input key values provided and passes the formatted string to <code>GPT4All</code>, <code>LLama-V2</code>, or another specified LLM.</p><p>In this case, the list of retrieved documents (<code>docs</code>) above are pass into <code>{context}</code>.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> PromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> LLMChain</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Prompt</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> PromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;Summarize the main themes in these retrieved docs: {docs}&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Chain</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">llm_chain </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> LLMChain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">llm</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> prompt</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Run</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">question </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;What are the approaches to Task Decomposition?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">docs </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> vectorstore</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">similarity_search</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">question</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">result </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> llm_chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">docs</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Output</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">result</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;text&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Llama.generate: prefix-match hit</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    Based on the retrieved documents, the main themes are:</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        load time =  1191.88 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:      sample time =   134.47 ms /   193 runs   (    0.70 ms per token,  1435.25 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings: prompt eval time = 39470.18 ms /  1055 tokens (   37.41 ms per token,    26.73 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        eval time =  8090.85 ms /   192 runs   (   42.14 ms per token,    23.73 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:       total time = 47943.12 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    &#x27;\nBased on the retrieved documents, the main themes are:\n1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system.\n2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner.\n3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence.\n4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.&#x27;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="qa-chain">QA Chain<a href="#qa-chain" class="hash-link" aria-label="Direct link to QA Chain" title="Direct link to QA Chain">‚Äã</a></h2><p>We can use a <code>QA chain</code> to handle our question above.</p><p><code>chain_type=&quot;stuff&quot;</code> (see <a href="https://python.langchain.com/docs/modules/chains/document/stuff" target="_blank" rel="noopener noreferrer">here</a>) means that all the docs will be added (stuffed) into a prompt.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">chains</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">question_answering </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> load_qa_chain</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Prompt</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">template </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">&quot;&quot;&quot;Use the following pieces of context to answer the question at the end. </span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">If you don&#x27;t know the answer, just say that you don&#x27;t know, don&#x27;t try to make up an answer. </span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">Use three sentences maximum and keep the answer as concise as possible. </span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">Always say &quot;thanks for asking!&quot; at the end of the answer. </span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">{context}</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">Question: {question}</span><br></span><span class="token-line" style="color:#000000"><span class="token triple-quoted-string string" style="color:rgb(163, 21, 21)">Helpful Answer:&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">QA_CHAIN_PROMPT </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> PromptTemplate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    input_variables</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;context&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;question&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    template</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Chain</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chain </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> load_qa_chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> chain_type</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;stuff&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> prompt</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">QA_CHAIN_PROMPT</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token comment" style="color:rgb(0, 128, 0)"># Run</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;input_documents&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> docs</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;question&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> question</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> return_only_outputs</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.question_answering.load_qa_chain.html"><span>load_qa_chain</span></a> <!-- -->from <code>langchain.chains.question_answering</code></li></ul></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Llama.generate: prefix-match hit</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">     Hi there! There are three main approaches to task decomposition. One is using LLM with simple prompting like &quot;Steps for XYZ.&quot; or &quot;What are the subgoals for achieving XYZ?&quot; Another approach is by using task-specific instructions, such as &quot;Write a story outline&quot; for writing a novel. Finally, task decomposition can also be done with human inputs. Thanks for asking!</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        load time =  1191.88 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:      sample time =    61.21 ms /    85 runs   (    0.72 ms per token,  1388.64 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings: prompt eval time =  8014.11 ms /   267 tokens (   30.02 ms per token,    33.32 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        eval time =  2908.17 ms /    84 runs   (   34.62 ms per token,    28.88 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:       total time = 11096.23 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    {&#x27;output_text&#x27;: &#x27; Hi there! There are three main approaches to task decomposition. One is using LLM with simple prompting like &quot;Steps for XYZ.&quot; or &quot;What are the subgoals for achieving XYZ?&quot; Another approach is by using task-specific instructions, such as &quot;Write a story outline&quot; for writing a novel. Finally, task decomposition can also be done with human inputs. Thanks for asking!&#x27;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="retrievalqa">RetrievalQA<a href="#retrievalqa" class="hash-link" aria-label="Direct link to RetrievalQA" title="Direct link to RetrievalQA">‚Äã</a></h2><p>For an even simpler flow, use <code>RetrievalQA</code>.</p><p>This will use a QA default prompt (shown <a href="https://github.com/hwchase17/langchain/blob/275b926cf745b5668d3ea30236635e20e7866442/langchain/chains/retrieval_qa/prompt.py#L4" target="_blank" rel="noopener noreferrer">here</a>) and will retrieve from the vectorDB.</p><p>But, you can still pass in a prompt, as before, if desired.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> langchain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">chains </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> RetrievalQA</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">qa_chain </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> RetrievalQA</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_chain_type</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llm</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    retriever</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain">vectorstore</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">as_retriever</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    chain_type_kwargs</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;prompt&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> QA_CHAIN_PROMPT</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div style="padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)"><h4 style="padding-left:0.65rem;margin-bottom:0.45rem">API Reference:</h4><ul style="padding-bottom:1rem"><li><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html"><span>RetrievalQA</span></a> <!-- -->from <code>langchain.chains</code></li></ul></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">qa_chain</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;query&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> question</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div lang="python"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#F5F5F5"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">    Llama.generate: prefix-match hit</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">     </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    The three approaches to Task decomposition are LLMs with simple prompting, task-specific instructions, or human inputs. Thanks for asking!</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    </span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        load time =  1191.88 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:      sample time =    22.78 ms /    31 runs   (    0.73 ms per token,  1360.66 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:        eval time =  1320.23 ms /    31 runs   (   42.59 ms per token,    23.48 tokens per second)</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    llama_print_timings:       total time =  1387.70 ms</span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain">    {&#x27;query&#x27;: &#x27;What are the approaches to Task Decomposition?&#x27;,</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">     &#x27;result&#x27;: &#x27; \nThe three approaches to Task decomposition are LLMs with simple prompting, task-specific instructions, or human inputs. Thanks for asking!&#x27;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/use_cases/question_answering/how_to/hyde"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Improve document indexing with HyDE</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/use_cases/question_answering/how_to/multi_retrieval_qa_router"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Dynamically select from multiple retrievers</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#document-loading" class="table-of-contents__link toc-highlight">Document Loading</a></li><li><a href="#model" class="table-of-contents__link toc-highlight">Model</a><ul><li><a href="#llama-v2" class="table-of-contents__link toc-highlight">Llama-v2</a></li><li><a href="#gpt4all" class="table-of-contents__link toc-highlight">GPT4All</a></li></ul></li><li><a href="#llmchain" class="table-of-contents__link toc-highlight">LLMChain</a></li><li><a href="#qa-chain" class="table-of-contents__link toc-highlight">QA Chain</a></li><li><a href="#retrievalqa" class="table-of-contents__link toc-highlight">RetrievalQA</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/cU2adEyC7w" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/LangChainAI" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/hwchase17/langchainjs" target="_blank" rel="noopener noreferrer" class="footer__link-item">JS/TS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://langchain.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Homepage<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://blog.langchain.dev" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2023 LangChain, Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.ba678a8c.js"></script>
<script src="/assets/js/main.ac0c2c80.js"></script>
</body>
</html>